{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Chapter 1 - Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     94145863998720,                  23,          4294967295,\n",
      "         8102871180449574772, 7957662828741227105],\n",
      "        [  32210688909071463,      94145864070960,                 193,\n",
      "              94145864103408,      94145863998848],\n",
      "        [     94149978095615, 7308324465835925846, 4856414708287165275,\n",
      "         6085614819301610832,                  64],\n",
      "        [                 64,      94145863998848,                  24,\n",
      "                  4294967295, 8102871180449574772],\n",
      "        [8027757794523574881, 8245936360722297970,     139701143910656,\n",
      "                          65,      94145864103408]])\n",
      "tensor([[-1.6692,  1.3385, -0.2117, -1.5619, -0.1039],\n",
      "        [-0.8950, -1.5265,  0.7751,  1.3009,  0.1460],\n",
      "        [ 2.0014,  1.9190,  0.9969, -0.0414, -0.4364],\n",
      "        [-0.8947,  1.8386, -0.1607,  0.0495,  0.8467],\n",
      "        [ 0.1548, -1.2458,  1.6172,  1.8410,  1.1961]])\n",
      "tensor([0.7015, 3.0434, 2.1644, 3.7635, 4.5006, 5.8220, 6.5683, 7.9355, 8.9220,\n",
      "        9.9383])\n",
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "A=torch.empty((5,5),dtype=torch.long)\n",
    "B=torch.randn((5,5),dtype=torch.float)\n",
    "C=torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n",
    "D=np.zeros((5,5),dtype=np.uint8)\n",
    "D=torch.tensor(D)\n",
    "print(A)\n",
    "print(B)\n",
    "print(C)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4146e+13, 2.4339e+01, 4.2950e+09, 8.1029e+18, 7.9577e+18],\n",
       "        [3.2211e+16, 9.4146e+13, 1.9378e+02, 9.4146e+13, 9.4146e+13],\n",
       "        [9.4150e+13, 7.3083e+18, 4.8564e+18, 6.0856e+18, 6.3564e+01],\n",
       "        [6.3105e+01, 9.4146e+13, 2.3839e+01, 4.2950e+09, 8.1029e+18],\n",
       "        [8.0278e+18, 8.2459e+18, 1.3970e+14, 6.6841e+01, 9.4146e+13]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=A+B\n",
    "E[:]\n",
    "#可索引 但是共享内存 修改一个都会变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量操作\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(*sizes)|全1Tensor|\n",
    "|zeros(*sizes)|全0Tensor|\n",
    "|eye(*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step)|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "\n",
    "The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions$ d, d+1, \\dots, d+kd,d+1,…,d+k $that satisfy the following contiguity-like condition that $\\forall i = 0, \\dots, k-1∀i=0,…,k−1 ,$\n",
    "$$\n",
    "\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
    "$$\n",
    "Otherwise, contiguous() needs to be called before the tensor can be viewed. See also: reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0922,  0.0418,  0.4470,  0.1606,  1.3236],\n",
      "        [ 1.9200,  1.3230,  1.4505,  2.0114,  0.0912],\n",
      "        [ 1.7515, -0.3208,  3.1449, -1.5550,  0.4567],\n",
      "        [-0.4851,  0.9575, -0.6078,  0.0127, -0.7880],\n",
      "        [ 0.1331,  1.0679,  0.0655,  1.1302,  0.9566]])\n",
      "tensor([-1.0922,  0.0418,  0.4470,  0.1606,  1.3236,  1.9200,  1.3230,  1.4505,\n",
      "         2.0114,  0.0912,  1.7515, -0.3208,  3.1449, -1.5550,  0.4567, -0.4851,\n",
      "         0.9575, -0.6078,  0.0127, -0.7880,  0.1331,  1.0679,  0.0655,  1.1302,\n",
      "         0.9566])\n",
      "max value index is  12\n"
     ]
    }
   ],
   "source": [
    "A=torch.randn((5,5))\n",
    "print(A)\n",
    "B=A.view(25)\n",
    "print(B)\n",
    "print(\"max value index is \",int(B.argmax(0).float().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量运算\n",
    "构造：\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(*sizes)|全1Tensor|\n",
    "|zeros(*sizes)|全0Tensor|\n",
    "|eye(*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step)|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "线性代数操作：\n",
    "\n",
    "| 函数\t|功能|\n",
    "|:---:|:---:|\n",
    "|trace|\t对角线元素之和(矩阵的迹)|\n",
    "|diag|\t对角线元素|\n",
    "|triu/tril\t|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm\t|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/baddbmm..|\t矩阵运算|\n",
    "|t|转置|\n",
    "|dot/cross|\t内积/外积|\n",
    "|inverse\t|求逆矩阵|\n",
    "|svd\t|奇异值分解|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动梯度\n",
    "autograd包能够根据输入和前向传播过程自动构建计算图，并执行反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1762,  0.2599, -1.7669], requires_grad=True)\n",
      "tensor([ 0.3748, -1.2066,  0.2440], requires_grad=True)\n",
      "tensor([-0.9962,  1.1738, -0.2715], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(3), requires_grad=True)\n",
    "y = Variable(torch.randn(3), requires_grad=True)\n",
    "z = Variable(torch.randn(3), requires_grad=True)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "t = x + y\n",
    "l = t.dot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![formular](https://www.zhihu.com/equation?tex=l+%3D+%28x%2By%29%5ETz%2C+dl%2Fdx+%3D+dl%2Fdy+%3D+z%2C+dl%2Fdz%3Dx%2By%3Dt%2C+dl%2Fdt%3Dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9962,  1.1738, -0.2715])\n",
      "tensor([-0.9962,  1.1738, -0.2715])\n",
      "tensor([-0.9962,  1.1738, -0.2715], requires_grad=True)\n",
      "tensor([ 0.1986, -0.9467, -1.5229])\n",
      "tensor([ 0.1986, -0.9467, -1.5229], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "print(y.grad) # x.grad = y.grad = z\n",
    "print(z)\n",
    "\n",
    "print(z.grad) # z.grad = t = x + y\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9962,  1.1738, -0.2715])\n",
      "tensor([-0.9962,  1.1738, -0.2715])\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_()\n",
    "y.grad.data.zero_()\n",
    "z.grad.data.zero_()\n",
    "\n",
    "t.backward(z) #dl/dt=z dl/z=dt\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：\n",
    "假设 x 经过一番计算得到 y，那么 y.backward(w) 求的不是 y 对 x 的导数，\n",
    "而是 l = torch.sum(y*w) 对 x 的导数。\n",
    "\n",
    "w 可以视为 y 的各分量的权重，也可以视为遥远的损失函数 l 对 y 的偏导数。\n",
    "\n",
    "也就是说，不一定需要从计算图最后的节点 y 往前反向传播，从中间某个节点 n 开始传也可以，只要你能把损失函数 l 关于这个节点的导数 dl/dn 记录下来，\n",
    "n.backward(dl/dn) 照样能往前回传，正确地计算出损失函数 l 对于节点 n 之前的节点的导数。\n",
    "\n",
    "特别地，若 y 为标量，w 取默认值 1.0，才是按照我们通常理解的那样，求 y 对 x 的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit",
   "language": "python",
   "name": "python36664bita0fe12396cdc43ecb420101ed1e6ff8b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
